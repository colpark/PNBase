{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# CIFAR-10 Extended NerfEmbedder: Super-Resolution Inference\n",
    "\n",
    "This notebook demonstrates the **Extended NerfEmbedder** architecture which improves NF quality through:\n",
    "\n",
    "## Key Innovations\n",
    "\n",
    "### 1. Extended Patch Boundaries (`margin=0.25`)\n",
    "- Standard: positions in `[0, 1]` → only see interior points\n",
    "- Extended: positions in `[-0.25, 1.25]` → 50% overlap at edges\n",
    "- Reduces seam artifacts at patch boundaries\n",
    "\n",
    "### 2. Position Jittering (`jitter_std=0.01`)\n",
    "- Training: `coord + N(0, 0.01)` noise added to positions\n",
    "- Forces model to learn continuous NF representations\n",
    "- Improves interpolation at arbitrary coordinates\n",
    "\n",
    "| Approach | Boundary Range | Position Jitter | Seam Artifacts |\n",
    "|----------|---------------|-----------------|----------------|\n",
    "| Original | [0, 1] | None | ❌ Visible |\n",
    "| **Extended** | [-0.25, 1.25] | ✅ Yes | ✅ Reduced |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Requires GPU\n",
    "- Assumes model was trained using `train_cifar10_extended.py`\n",
    "- Update `CKPT_PATH` to point to your trained checkpoint\n",
    "\n",
    "### Training command:\n",
    "```bash\n",
    "python train_cifar10_extended.py --margin 0.25 --jitter_std 0.01 --max_steps 100000\n",
    "```\n",
    "\n",
    "### Model config (must match training - ~10M params):\n",
    "| Parameter | Value | Why |\n",
    "|-----------|-------|-----|\n",
    "| patch_size | 2 | 16×16=256 encoder tokens |\n",
    "| margin | 0.25 | Predict [-0.25, 1.25] for overlap |\n",
    "| jitter_std | 0.01 | Position noise during training |\n",
    "| hidden_size | 256 | ~10M params |\n",
    "| decoder_hidden_size | 32 | Decoder capacity |\n",
    "| num_encoder_blocks | 6 | Encoder depth |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to PixNerd folder where src/ is located\n",
    "import os\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "print(f\"Starting directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Navigate to PixNerd folder (where src/ lives)\n",
    "PIXNERD_DIR = os.path.join(NOTEBOOK_DIR, \"PixNerd\")\n",
    "if os.path.exists(PIXNERD_DIR):\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "elif os.path.basename(NOTEBOOK_DIR) == \"PixNerd\":\n",
    "    print(f\"Already in PixNerd directory: {NOTEBOOK_DIR}\")\n",
    "else:\n",
    "    parent = os.path.dirname(NOTEBOOK_DIR)\n",
    "    pixnerd_in_parent = os.path.join(parent, \"PixNerd\")\n",
    "    if os.path.exists(pixnerd_in_parent):\n",
    "        os.chdir(pixnerd_in_parent)\n",
    "        print(f\"Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Could not find PixNerd folder. Current dir: {NOTEBOOK_DIR}\")\n",
    "\n",
    "if os.path.exists(\"src\"):\n",
    "    print(\"Found src/ directory\")\n",
    "else:\n",
    "    print(\"ERROR: src/ directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "PIXNERD_ROOT = Path(os.getcwd())\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINT PATH - UPDATE THIS TO YOUR TRAINED MODEL\n",
    "# ============================================================\n",
    "CKPT_PATH = PIXNERD_ROOT / \"workdirs\" / \"exp_cifar10_extended_nerf\" / \"checkpoints\" / \"last.ckpt\"\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_DIR = PIXNERD_ROOT / \"outputs\" / \"cifar10_extended_superres\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE != \"cuda\":\n",
    "    print(\"WARNING: Running on CPU will be very slow\")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# MODEL CONFIG - Must match train_cifar10_extended.py (~10M params)\n",
    "# ============================================================\n",
    "NUM_CLASSES = 10\n",
    "BASE_RES = 32  # CIFAR-10 native resolution\n",
    "\n",
    "PATCH_SIZE = 2           # Controls encoder tokens: 32/2 = 16x16 = 256 tokens\n",
    "\n",
    "# EXTENDED NERFEMBEDDER CONFIG - KEY INNOVATIONS!\n",
    "MARGIN = 0.25            # Predict [-0.25, 1.25] for overlapping patches\n",
    "JITTER_STD = 0.01        # Position jittering during training\n",
    "\n",
    "# Model dimensions (~10M parameters)\n",
    "HIDDEN_SIZE = 256\n",
    "DECODER_HIDDEN_SIZE = 32\n",
    "NUM_ENCODER_BLOCKS = 6\n",
    "NUM_DECODER_BLOCKS = 2\n",
    "NUM_GROUPS = 4\n",
    "# ============================================================\n",
    "\n",
    "print(f\"PixNerd root: {PIXNERD_ROOT}\")\n",
    "print(f\"Checkpoint path: {CKPT_PATH}\")\n",
    "print(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\n",
    "if not CKPT_PATH.exists():\n",
    "    print(f\"\\n⚠️  CHECKPOINT NOT FOUND!\")\n",
    "    print(f\"   Please train a model first using:\")\n",
    "    print(f\"   python train_cifar10_extended.py --margin {MARGIN} --jitter_std {JITTER_STD}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTENDED NERFEMBEDDER ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Extended boundaries: margin={MARGIN}\")\n",
    "print(f\"    → Positions span [{-MARGIN:.2f}, {1+MARGIN:.2f}] instead of [0, 1]\")\n",
    "print(f\"    → Reduces seam artifacts at patch boundaries\")\n",
    "print()\n",
    "print(f\"  Position jittering: std={JITTER_STD}\")\n",
    "print(f\"    → Gaussian noise during training\")\n",
    "print(f\"    → Forces continuous NF learning\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "Using `PixNerDiTExtended` with extended boundaries and jittering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PixNerd components - NOTE: Using EXTENDED model!\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_extended import PixNerDiTExtended  # Extended version!\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training import FlowMatchingTrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "from src.models.autoencoder.base import fp2uint8\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(\"Using: PixNerDiTExtended (Extended Boundaries + Position Jittering)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model components...\")\n",
    "\n",
    "main_scheduler = LinearScheduler()\n",
    "\n",
    "vae = PixelAE(scale=1.0)\n",
    "\n",
    "conditioner = LabelConditioner(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Extended NerfEmbedder model - KEY DIFFERENCE!\n",
    "denoiser = PixNerDiTExtended(\n",
    "    in_channels=3,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_groups=NUM_GROUPS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    decoder_hidden_size=DECODER_HIDDEN_SIZE,\n",
    "    num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
    "    num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    margin=MARGIN,              # Extended boundaries!\n",
    "    jitter_std=JITTER_STD,      # Position jittering (only during training)\n",
    ")\n",
    "\n",
    "# Sampler with CFG\n",
    "sampler = EulerSampler(\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    "    guidance_interval_min=0.0,\n",
    "    guidance_interval_max=1.0,\n",
    "    scheduler=main_scheduler,\n",
    "    w_scheduler=LinearScheduler(),\n",
    "    guidance_fn=simple_guidance_fn,\n",
    "    step_fn=ode_step_fn,\n",
    ")\n",
    "\n",
    "# Trainer stub for checkpoint loading\n",
    "trainer_stub = FlowMatchingTrainer(\n",
    "    scheduler=main_scheduler,\n",
    "    lognorm_t=True,\n",
    "    timeshift=1.0,\n",
    ")\n",
    "\n",
    "ema_tracker = SimpleEMA(decay=0.9999)\n",
    "\n",
    "model = LightningModel(\n",
    "    vae=vae,\n",
    "    conditioner=conditioner,\n",
    "    denoiser=denoiser,\n",
    "    diffusion_trainer=trainer_stub,\n",
    "    diffusion_sampler=sampler,\n",
    "    ema_tracker=ema_tracker,\n",
    "    optimizer=None,\n",
    "    lr_scheduler=None,\n",
    "    eval_original_model=False,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(f\"Model initialized and moved to {DEVICE}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "print(f\"Architecture: PixNerDiTExtended\")\n",
    "print(f\"  - margin={MARGIN} → positions in [{-MARGIN:.2f}, {1+MARGIN:.2f}]\")\n",
    "print(f\"  - jitter_std={JITTER_STD} (only during training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading checkpoint from: {CKPT_PATH}\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
    "if missing:\n",
    "    print(f\"  Missing: {missing[:5]}...\" if len(missing) > 5 else f\"  Missing: {missing}\")\n",
    "if unexpected:\n",
    "    print(f\"  Unexpected: {unexpected[:5]}...\" if len(unexpected) > 5 else f\"  Unexpected: {unexpected}\")\n",
    "print(\"Checkpoint loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1p2q3r4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_scale(scale: float):\n",
    "    \"\"\"Set NF decoder patch scaling for super-resolution.\"\"\"\n",
    "    for net in [model.denoiser, getattr(model, \"ema_denoiser\", None)]:\n",
    "        if net is None:\n",
    "            continue\n",
    "        net.decoder_patch_scaling_h = scale\n",
    "        net.decoder_patch_scaling_w = scale\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_class_conditional(\n",
    "    class_labels: list,\n",
    "    height: int = 32,\n",
    "    width: int = 32,\n",
    "    seed: int = 42,\n",
    "    num_steps: int = 50,\n",
    "    guidance: float = 2.0,\n",
    "    base_res: int = BASE_RES,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate class-conditional images.\n",
    "    \n",
    "    Args:\n",
    "        class_labels: List of class indices (0-9) or names\n",
    "        height: Output height (32 for native, 128 for 4x super-res)\n",
    "        width: Output width\n",
    "        seed: Random seed\n",
    "        num_steps: ODE solver steps\n",
    "        guidance: CFG guidance scale\n",
    "        base_res: Training resolution\n",
    "    \n",
    "    Returns:\n",
    "        Generated images as uint8 tensor\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Convert class names to indices if needed\n",
    "    labels = []\n",
    "    for label in class_labels:\n",
    "        if isinstance(label, str):\n",
    "            label = CIFAR10_CLASSES.index(label.lower())\n",
    "        labels.append(label)\n",
    "    \n",
    "    batch_size = len(labels)\n",
    "    \n",
    "    # Set decoder scale for super-resolution\n",
    "    if height == base_res and width == base_res:\n",
    "        set_decoder_scale(1.0)\n",
    "        print(f\"Generating at native {base_res}x{base_res}\")\n",
    "    else:\n",
    "        scale_h = height / float(base_res)\n",
    "        scale_w = width / float(base_res)\n",
    "        assert scale_h == scale_w, \"Only square scaling supported\"\n",
    "        set_decoder_scale(scale_h)\n",
    "        print(f\"Generating at {height}x{width} ({scale_h:.0f}x super-resolution)\")\n",
    "    \n",
    "    # Configure sampler\n",
    "    model.diffusion_sampler.guidance = guidance\n",
    "    model.diffusion_sampler.num_steps = num_steps\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn(batch_size, 3, height, width, device=DEVICE)\n",
    "    \n",
    "    # Get condition and uncondition\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    condition = condition.to(DEVICE)\n",
    "    uncondition = uncondition.to(DEVICE)\n",
    "    \n",
    "    # Sample\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    images = model.vae.decode(samples)\n",
    "    images = torch.clamp(images, -1.0, 1.0)\n",
    "    images_uint8 = fp2uint8(images)\n",
    "    \n",
    "    return images_uint8.cpu()\n",
    "\n",
    "\n",
    "def show_images(images_uint8, labels=None, title=\"\", cols=None):\n",
    "    \"\"\"Display a batch of images with labels.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    n = len(imgs_np)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs_np)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if labels is not None:\n",
    "            label = labels[i]\n",
    "            if isinstance(label, int):\n",
    "                label = CIFAR10_CLASSES[label]\n",
    "            ax.set_title(label)\n",
    "    \n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_grid(images_uint8, filename, labels=None, cols=None):\n",
    "    \"\"\"Save images as a grid.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    imgs = [Image.fromarray(img) for img in imgs_np]\n",
    "    \n",
    "    n = len(imgs)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", (cols * w, rows * h))\n",
    "    for idx, img in enumerate(imgs):\n",
    "        r, c = divmod(idx, cols)\n",
    "        grid.paste(img, (c * w, r * h))\n",
    "    \n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    grid.save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8",
   "metadata": {},
   "source": [
    "## Generate at Native Resolution (32x32)\n",
    "\n",
    "Generate one sample per class at the native CIFAR-10 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9x0y1z2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 32x32\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate one image per class\n",
    "all_classes = list(range(10))\n",
    "\n",
    "images_32 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=32,\n",
    "    width=32,\n",
    "    seed=42,\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_32.shape}\")\n",
    "show_images(images_32, labels=CIFAR10_CLASSES, title=\"Extended NerfEmbedder: CIFAR-10 at 32x32\")\n",
    "save_grid(images_32, \"extended_cifar10_32x32_all_classes.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Generate at 4x Super-Resolution (128x128)\n",
    "\n",
    "The Extended NerfEmbedder should produce smoother results due to:\n",
    "1. **Extended boundaries** - patches overlap, reducing seam artifacts\n",
    "2. **Position jittering during training** - learned continuous representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 128x128 (4x super-resolution)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_128 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=128,\n",
    "    width=128,\n",
    "    seed=42,  # Same seed for comparison\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_128.shape}\")\n",
    "show_images(images_128, labels=CIFAR10_CLASSES, title=\"Extended NerfEmbedder: CIFAR-10 at 128x128 (4x Super-Res)\")\n",
    "save_grid(images_128, \"extended_cifar10_128x128_4x_superres.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison: 32x32 vs 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5n6o7p8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific classes\n",
    "comparison_classes = ['cat', 'dog', 'airplane', 'ship']\n",
    "comparison_labels = [CIFAR10_CLASSES.index(c) for c in comparison_classes]\n",
    "\n",
    "print(\"Comparing 32x32 vs Bilinear 128x128 vs Extended NF 128x128\")\n",
    "\n",
    "# Generate at 32x32\n",
    "imgs_32 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=32, width=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Generate at 128x128 (4x)\n",
    "imgs_128 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=128, width=128,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Bilinear upscale 32->128\n",
    "imgs_32_upscaled = F.interpolate(\n",
    "    imgs_32.float() / 255.0,\n",
    "    size=(128, 128),\n",
    "    mode='bilinear',\n",
    "    align_corners=False,\n",
    ")\n",
    "imgs_32_upscaled = (imgs_32_upscaled * 255).to(torch.uint8)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(len(comparison_classes), 3, figsize=(12, 4 * len(comparison_classes)))\n",
    "\n",
    "for i, class_name in enumerate(comparison_classes):\n",
    "    # 32x32 native\n",
    "    axes[i, 0].imshow(imgs_32[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title(f\"{class_name} - 32x32 Native\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Bilinear upscale\n",
    "    axes[i, 1].imshow(imgs_32_upscaled[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title(f\"{class_name} - Bilinear 128x128\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Extended NF super-res\n",
    "    axes[i, 2].imshow(imgs_128[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 2].set_title(f\"{class_name} - Extended NF 128x128\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"32x32 Native vs Bilinear Upscale vs Extended NF Super-Resolution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9r0s1t2",
   "metadata": {},
   "source": [
    "## Try Different Super-Resolution Scales\n",
    "\n",
    "Test the extended boundaries at various scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scales: 1x, 2x, 4x, 8x\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different super-resolution scales\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scales = [\n",
    "    (32, \"1x (Native)\"),\n",
    "    (64, \"2x\"),\n",
    "    (128, \"4x\"),\n",
    "    (256, \"8x\"),\n",
    "]\n",
    "\n",
    "class_label = 'airplane'\n",
    "seed = 42\n",
    "\n",
    "scale_images = []\n",
    "scale_titles = []\n",
    "\n",
    "for resolution, scale_name in scales:\n",
    "    print(f\"Generating at {resolution}x{resolution}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=resolution, width=resolution,\n",
    "        seed=seed,\n",
    "    )\n",
    "    scale_images.append(img[0])\n",
    "    scale_titles.append(f\"{resolution}x{resolution} ({scale_name})\")\n",
    "\n",
    "# Display at same visual size\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (img, title) in enumerate(zip(scale_images, scale_titles)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} at Different Resolutions (Extended NerfEmbedder)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boundary_analysis",
   "metadata": {},
   "source": [
    "## Boundary Artifact Analysis\n",
    "\n",
    "Let's look closely at potential boundary artifacts.\n",
    "The extended boundaries should reduce visible seams between patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boundary_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a high-res image and zoom in on potential boundary regions\n",
    "print(\"=\" * 50)\n",
    "print(\"Boundary artifact analysis at 256x256\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "img_256 = sample_class_conditional(\n",
    "    class_labels=['cat'],\n",
    "    height=256, width=256,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "img_np = img_256[0].permute(1, 2, 0).numpy()\n",
    "\n",
    "# Show full image and zoomed regions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Full image\n",
    "axes[0, 0].imshow(img_np)\n",
    "axes[0, 0].set_title(\"Full 256x256 Image\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Add grid lines showing patch boundaries (every 16 pixels at 8x scale)\n",
    "axes[0, 1].imshow(img_np)\n",
    "patch_size_scaled = PATCH_SIZE * 8  # 2 * 8 = 16 at 256x256\n",
    "for i in range(0, 257, patch_size_scaled):\n",
    "    axes[0, 1].axhline(y=i, color='red', linewidth=0.5, alpha=0.5)\n",
    "    axes[0, 1].axvline(x=i, color='red', linewidth=0.5, alpha=0.5)\n",
    "axes[0, 1].set_title(\"With Patch Boundaries (red)\")\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Zoom regions\n",
    "zoom_regions = [\n",
    "    (64, 64, 128, 128, \"Center region\"),\n",
    "    (0, 0, 64, 64, \"Top-left corner\"),\n",
    "    (192, 192, 256, 256, \"Bottom-right corner\"),\n",
    "    (112, 112, 144, 144, \"Patch boundary zone\"),\n",
    "]\n",
    "\n",
    "for idx, (y1, x1, y2, x2, title) in enumerate(zoom_regions[:4]):\n",
    "    ax = axes.flatten()[idx + 2]\n",
    "    ax.imshow(img_np[y1:y2, x1:x2])\n",
    "    ax.set_title(f\"Zoom: {title}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Boundary Artifact Analysis - Extended NerfEmbedder\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "save_grid(img_256, \"extended_cat_256x256_8x_superres.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "## Guidance Scale Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0m1n2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different CFG guidance scales at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "guidance_scales = [1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "class_label = 'horse'\n",
    "seed = 42\n",
    "\n",
    "guidance_images = []\n",
    "for g in guidance_scales:\n",
    "    print(f\"Guidance = {g}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=128, width=128,\n",
    "        seed=seed,\n",
    "        guidance=g,\n",
    "    )\n",
    "    guidance_images.append(img[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, g) in enumerate(zip(guidance_images, guidance_scales)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f\"guidance={g}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} with Different CFG Scales (Extended NerfEmbedder)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variety_samples",
   "metadata": {},
   "source": [
    "## Generate Variety of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variety_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 different samples for selected classes at 128x128\n",
    "print(\"=\" * 50)\n",
    "print(\"Generating 5 samples per class at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "selected_classes = ['cat', 'dog', 'airplane']\n",
    "seeds = [0, 42, 123, 456, 789]\n",
    "\n",
    "for class_name in selected_classes:\n",
    "    print(f\"\\nGenerating {class_name}s...\")\n",
    "    class_images = []\n",
    "    for seed in seeds:\n",
    "        img = sample_class_conditional(\n",
    "            class_labels=[class_name],\n",
    "            height=128, width=128,\n",
    "            seed=seed,\n",
    "        )\n",
    "        class_images.append(img)\n",
    "    \n",
    "    class_images = torch.cat(class_images, dim=0)\n",
    "    show_images(class_images, labels=[class_name] * 5, title=f\"5 {class_name.capitalize()}s at 128x128 (Extended NF)\")\n",
    "    save_grid(class_images, f\"extended_{class_name}s_128x128.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **Extended NerfEmbedder** architecture:\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "| Feature | Description | Benefit |\n",
    "|---------|-------------|--------|\n",
    "| **Extended Boundaries** | Positions in [-0.25, 1.25] | Overlapping patches reduce seams |\n",
    "| **Position Jittering** | N(0, 0.01) noise during training | Smoother NF interpolation |\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**Training:**\n",
    "- NerfEmbedder predicts positions beyond [0,1] → sees \"outside\" each patch\n",
    "- Random jitter forces continuous representation learning\n",
    "\n",
    "**Inference:**\n",
    "- Same extended positions, no jittering\n",
    "- Overlapping regions can be blended for seamless output\n",
    "\n",
    "### Comparison with Other Approaches\n",
    "\n",
    "| Approach | Boundary | Jitter | Seam Artifacts | Interpolation |\n",
    "|----------|----------|--------|----------------|---------------|\n",
    "| Original | [0,1] | None | ❌ Visible | ⚠️ Grid-locked |\n",
    "| Multi-scale | [0,1] + dense | None | ⚠️ Some | ✅ Better |\n",
    "| **Extended** | [-0.25,1.25] | ✅ Yes | ✅ Reduced | ✅ Smooth |\n",
    "\n",
    "### Training Command\n",
    "```bash\n",
    "python train_cifar10_extended.py \\\n",
    "    --margin 0.25 \\\n",
    "    --jitter_std 0.01 \\\n",
    "    --max_steps 100000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7t8u9v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
