{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# CIFAR-10 Class-Conditional Generation with 4x Super-Resolution\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading a trained CIFAR-10 class-conditional PixNerd model\n",
    "2. Generating images at native 32x32 resolution\n",
    "3. Generating 4x super-resolution images at 128x128\n",
    "\n",
    "The model uses the heavy decoder architecture with NerfBlocks that enables\n",
    "arbitrary resolution output via continuous coordinate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Requires GPU\n",
    "- Assumes model was trained using `configs_cifar10/cifar10_c2i_heavydecoder_simple.yaml`\n",
    "- Update `CKPT_PATH` to point to your trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to PixNerd folder where src/ is located\n",
    "import os\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "print(f\"Starting directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Navigate to PixNerd folder (where src/ lives)\n",
    "PIXNERD_DIR = os.path.join(NOTEBOOK_DIR, \"PixNerd\")\n",
    "if os.path.exists(PIXNERD_DIR):\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "elif os.path.basename(NOTEBOOK_DIR) == \"PixNerd\":\n",
    "    print(f\"Already in PixNerd directory: {NOTEBOOK_DIR}\")\n",
    "else:\n",
    "    parent = os.path.dirname(NOTEBOOK_DIR)\n",
    "    pixnerd_in_parent = os.path.join(parent, \"PixNerd\")\n",
    "    if os.path.exists(pixnerd_in_parent):\n",
    "        os.chdir(pixnerd_in_parent)\n",
    "        print(f\"Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Could not find PixNerd folder. Current dir: {NOTEBOOK_DIR}\")\n",
    "\n",
    "if os.path.exists(\"src\"):\n",
    "    print(\"Found src/ directory\")\n",
    "else:\n",
    "    print(\"ERROR: src/ directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "PIXNERD_ROOT = Path(os.getcwd())\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINT PATH - UPDATE THIS TO YOUR TRAINED MODEL\n",
    "# ============================================================\n",
    "CKPT_PATH = PIXNERD_ROOT / \"workdirs\" / \"exp_cifar10_c2i_heavydecoder_simple\" / \"last.ckpt\"\n",
    "# Alternative paths to try:\n",
    "# CKPT_PATH = Path(\"/path/to/your/checkpoint.ckpt\")\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_DIR = PIXNERD_ROOT / \"outputs\" / \"cifar10_superres\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE != \"cuda\":\n",
    "    print(\"WARNING: Running on CPU will be very slow\")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# Model config (must match training)\n",
    "NUM_CLASSES = 10\n",
    "BASE_RES = 32  # CIFAR-10 native resolution\n",
    "HIDDEN_SIZE = 512\n",
    "DECODER_HIDDEN_SIZE = 64\n",
    "NUM_ENCODER_BLOCKS = 12\n",
    "NUM_DECODER_BLOCKS = 2\n",
    "PATCH_SIZE = 2\n",
    "NUM_GROUPS = 8\n",
    "\n",
    "print(f\"PixNerd root: {PIXNERD_ROOT}\")\n",
    "print(f\"Checkpoint path: {CKPT_PATH}\")\n",
    "print(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\n",
    "if not CKPT_PATH.exists():\n",
    "    print(f\"\\n⚠️  CHECKPOINT NOT FOUND!\")\n",
    "    print(f\"   Please train a model first using:\")\n",
    "    print(f\"   python main.py fit -c configs_cifar10/cifar10_c2i_heavydecoder_simple.yaml\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PixNerd components\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training import FlowMatchingTrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "from src.models.autoencoder.base import fp2uint8\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model components...\")\n",
    "\n",
    "main_scheduler = LinearScheduler()\n",
    "\n",
    "vae = PixelAE(scale=1.0)\n",
    "\n",
    "conditioner = LabelConditioner(num_classes=NUM_CLASSES)\n",
    "\n",
    "denoiser = PixNerDiT(\n",
    "    in_channels=3,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_groups=NUM_GROUPS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    decoder_hidden_size=DECODER_HIDDEN_SIZE,\n",
    "    num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
    "    num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "# Sampler with CFG\n",
    "sampler = EulerSampler(\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    "    guidance_interval_min=0.0,\n",
    "    guidance_interval_max=1.0,\n",
    "    scheduler=main_scheduler,\n",
    "    w_scheduler=LinearScheduler(),\n",
    "    guidance_fn=simple_guidance_fn,\n",
    "    step_fn=ode_step_fn,\n",
    ")\n",
    "\n",
    "# Trainer stub for checkpoint loading\n",
    "trainer_stub = FlowMatchingTrainer(\n",
    "    scheduler=main_scheduler,\n",
    "    lognorm_t=True,\n",
    "    timeshift=1.0,\n",
    ")\n",
    "\n",
    "ema_tracker = SimpleEMA(decay=0.9999)\n",
    "\n",
    "model = LightningModel(\n",
    "    vae=vae,\n",
    "    conditioner=conditioner,\n",
    "    denoiser=denoiser,\n",
    "    diffusion_trainer=trainer_stub,\n",
    "    diffusion_sampler=sampler,\n",
    "    ema_tracker=ema_tracker,\n",
    "    optimizer=None,\n",
    "    lr_scheduler=None,\n",
    "    eval_original_model=False,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(f\"Model initialized and moved to {DEVICE}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading checkpoint from: {CKPT_PATH}\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
    "print(\"Checkpoint loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1p2q3r4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_scale(scale: float):\n",
    "    \"\"\"Set NF decoder patch scaling for super-resolution.\"\"\"\n",
    "    for net in [model.denoiser, getattr(model, \"ema_denoiser\", None)]:\n",
    "        if net is None:\n",
    "            continue\n",
    "        net.decoder_patch_scaling_h = scale\n",
    "        net.decoder_patch_scaling_w = scale\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_class_conditional(\n",
    "    class_labels: list,\n",
    "    height: int = 32,\n",
    "    width: int = 32,\n",
    "    seed: int = 42,\n",
    "    num_steps: int = 50,\n",
    "    guidance: float = 2.0,\n",
    "    base_res: int = BASE_RES,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate class-conditional images.\n",
    "    \n",
    "    Args:\n",
    "        class_labels: List of class indices (0-9) or names\n",
    "        height: Output height (32 for native, 128 for 4x super-res)\n",
    "        width: Output width\n",
    "        seed: Random seed\n",
    "        num_steps: ODE solver steps\n",
    "        guidance: CFG guidance scale\n",
    "        base_res: Training resolution\n",
    "    \n",
    "    Returns:\n",
    "        Generated images as uint8 tensor\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Convert class names to indices if needed\n",
    "    labels = []\n",
    "    for label in class_labels:\n",
    "        if isinstance(label, str):\n",
    "            label = CIFAR10_CLASSES.index(label.lower())\n",
    "        labels.append(label)\n",
    "    \n",
    "    batch_size = len(labels)\n",
    "    \n",
    "    # Set decoder scale for super-resolution\n",
    "    if height == base_res and width == base_res:\n",
    "        set_decoder_scale(1.0)\n",
    "        print(f\"Generating at native {base_res}x{base_res}\")\n",
    "    else:\n",
    "        scale_h = height / float(base_res)\n",
    "        scale_w = width / float(base_res)\n",
    "        assert scale_h == scale_w, \"Only square scaling supported\"\n",
    "        set_decoder_scale(scale_h)\n",
    "        print(f\"Generating at {height}x{width} ({scale_h:.0f}x super-resolution)\")\n",
    "    \n",
    "    # Configure sampler\n",
    "    model.diffusion_sampler.guidance = guidance\n",
    "    model.diffusion_sampler.num_steps = num_steps\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn(batch_size, 3, height, width, device=DEVICE)\n",
    "    \n",
    "    # Get condition and uncondition\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    condition = condition.to(DEVICE)\n",
    "    uncondition = uncondition.to(DEVICE)\n",
    "    \n",
    "    # Sample\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    images = model.vae.decode(samples)\n",
    "    images = torch.clamp(images, -1.0, 1.0)\n",
    "    images_uint8 = fp2uint8(images)\n",
    "    \n",
    "    return images_uint8.cpu()\n",
    "\n",
    "\n",
    "def show_images(images_uint8, labels=None, title=\"\", cols=None):\n",
    "    \"\"\"Display a batch of images with labels.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    n = len(imgs_np)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs_np)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if labels is not None:\n",
    "            label = labels[i]\n",
    "            if isinstance(label, int):\n",
    "                label = CIFAR10_CLASSES[label]\n",
    "            ax.set_title(label)\n",
    "    \n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_grid(images_uint8, filename, labels=None, cols=None):\n",
    "    \"\"\"Save images as a grid.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    imgs = [Image.fromarray(img) for img in imgs_np]\n",
    "    \n",
    "    n = len(imgs)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", (cols * w, rows * h))\n",
    "    for idx, img in enumerate(imgs):\n",
    "        r, c = divmod(idx, cols)\n",
    "        grid.paste(img, (c * w, r * h))\n",
    "    \n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    grid.save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8",
   "metadata": {},
   "source": [
    "## Generate at Native Resolution (32x32)\n",
    "\n",
    "Generate one sample per class at the native CIFAR-10 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9x0y1z2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 32x32\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate one image per class\n",
    "all_classes = list(range(10))\n",
    "\n",
    "images_32 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=32,\n",
    "    width=32,\n",
    "    seed=42,\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_32.shape}\")\n",
    "show_images(images_32, labels=CIFAR10_CLASSES, title=\"CIFAR-10 at 32x32\")\n",
    "save_grid(images_32, \"cifar10_32x32_all_classes.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Generate at 4x Super-Resolution (128x128)\n",
    "\n",
    "The heavy decoder architecture enables generating at higher resolutions\n",
    "using the same model weights via continuous coordinate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 128x128 (4x super-resolution)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_128 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=128,\n",
    "    width=128,\n",
    "    seed=42,  # Same seed for comparison\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_128.shape}\")\n",
    "show_images(images_128, labels=CIFAR10_CLASSES, title=\"CIFAR-10 at 128x128 (4x Super-Res)\")\n",
    "save_grid(images_128, \"cifar10_128x128_4x_superres.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison: 32x32 vs 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5n6o7p8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific classes\n",
    "comparison_classes = ['cat', 'dog', 'airplane', 'ship']\n",
    "comparison_labels = [CIFAR10_CLASSES.index(c) for c in comparison_classes]\n",
    "\n",
    "print(\"Comparing 32x32 vs Bilinear 128x128 vs NF 128x128\")\n",
    "\n",
    "# Generate at 32x32\n",
    "imgs_32 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=32, width=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Generate at 128x128 (4x)\n",
    "imgs_128 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=128, width=128,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Bilinear upscale 32->128\n",
    "imgs_32_upscaled = F.interpolate(\n",
    "    imgs_32.float() / 255.0,\n",
    "    size=(128, 128),\n",
    "    mode='bilinear',\n",
    "    align_corners=False,\n",
    ")\n",
    "imgs_32_upscaled = (imgs_32_upscaled * 255).to(torch.uint8)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(len(comparison_classes), 3, figsize=(12, 4 * len(comparison_classes)))\n",
    "\n",
    "for i, class_name in enumerate(comparison_classes):\n",
    "    # 32x32 native\n",
    "    axes[i, 0].imshow(imgs_32[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title(f\"{class_name} - 32x32 Native\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Bilinear upscale\n",
    "    axes[i, 1].imshow(imgs_32_upscaled[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title(f\"{class_name} - Bilinear 128x128\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # NF super-res\n",
    "    axes[i, 2].imshow(imgs_128[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 2].set_title(f\"{class_name} - NF 128x128 (4x)\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"32x32 Native vs Bilinear Upscale vs NF Super-Resolution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9r0s1t2",
   "metadata": {},
   "source": [
    "## Generate Multiple Samples per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u3v4w5x6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 cats at different seeds\n",
    "print(\"=\" * 50)\n",
    "print(\"Generating 5 cats with different seeds at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cat_images = []\n",
    "for seed in [0, 42, 123, 456, 789]:\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=['cat'],\n",
    "        height=128, width=128,\n",
    "        seed=seed,\n",
    "    )\n",
    "    cat_images.append(img)\n",
    "\n",
    "cat_images = torch.cat(cat_images, dim=0)\n",
    "show_images(cat_images, labels=['cat'] * 5, title=\"5 Cats at 128x128\")\n",
    "save_grid(cat_images, \"cifar10_cats_128x128.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7z8a9b0",
   "metadata": {},
   "source": [
    "## Try Different Super-Resolution Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scales: 1x, 2x, 4x, 8x\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different super-resolution scales\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scales = [\n",
    "    (32, \"1x (Native)\"),\n",
    "    (64, \"2x\"),\n",
    "    (128, \"4x\"),\n",
    "    (256, \"8x\"),\n",
    "]\n",
    "\n",
    "class_label = 'airplane'\n",
    "seed = 42\n",
    "\n",
    "scale_images = []\n",
    "scale_titles = []\n",
    "\n",
    "for resolution, scale_name in scales:\n",
    "    print(f\"Generating at {resolution}x{resolution}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=resolution, width=resolution,\n",
    "        seed=seed,\n",
    "    )\n",
    "    scale_images.append(img[0])\n",
    "    scale_titles.append(f\"{resolution}x{resolution} ({scale_name})\")\n",
    "\n",
    "# Display at same visual size\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (img, title) in enumerate(zip(scale_images, scale_titles)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} at Different Resolutions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "## Guidance Scale Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0m1n2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different CFG guidance scales at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "guidance_scales = [1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "class_label = 'horse'\n",
    "seed = 42\n",
    "\n",
    "guidance_images = []\n",
    "for g in guidance_scales:\n",
    "    print(f\"Guidance = {g}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=128, width=128,\n",
    "        seed=seed,\n",
    "        guidance=g,\n",
    "    )\n",
    "    guidance_images.append(img[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, g) in enumerate(zip(guidance_images, guidance_scales)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f\"guidance={g}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} with Different CFG Scales\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "### Class-Conditional Generation\n",
    "- Generate images conditioned on CIFAR-10 class labels (0-9)\n",
    "- Classifier-Free Guidance (CFG) for improved sample quality\n",
    "\n",
    "### Super-Resolution via Heavy Decoder\n",
    "- **Native resolution**: 32x32 (CIFAR-10 training resolution)\n",
    "- **4x super-resolution**: 128x128 with same model weights\n",
    "- **Arbitrary scales**: 2x, 4x, 8x, etc.\n",
    "\n",
    "### How Super-Resolution Works\n",
    "The heavy decoder uses:\n",
    "1. **NerfEmbedder**: Continuous coordinate embeddings normalized to [0,1]\n",
    "2. **Hypernetwork NerfBlocks**: Encoder generates MLP weights that process arbitrary pixel counts\n",
    "3. **decoder_patch_scaling**: Adjusts decoder patch size to handle higher resolutions\n",
    "\n",
    "### Key Parameters\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `guidance` | CFG scale | 1.5-3.0 |\n",
    "| `num_steps` | ODE solver steps | 25-100 |\n",
    "| `height/width` | Output resolution | 32, 64, 128, 256 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7t8u9v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
