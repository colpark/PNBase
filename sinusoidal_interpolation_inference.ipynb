{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Sinusoidal Neural Field Interpolation Evaluation\n\nThis notebook evaluates PixNerd's neural field interpolation capabilities for super-resolution.\n\n## Benchmark Design\n\n- **Training**: Model sees regularly sampled pixels (simulating low-res input)\n- **Testing**: Model must predict ALL pixels (super-resolution)\n- **Ground Truth**: Known sinusoidal patterns (smooth, continuous)\n\n### Super-Resolution Simulation\nRegular grid sampling simulates a downsampled low-resolution input:\n- `downsample_factor=4`: Every 4th pixel visible = 4x super-res (6.25% visible in 2D)\n- `downsample_factor=2`: Every 2nd pixel visible = 2x super-res (25% visible in 2D)\n\n### Key Metrics\n1. **Visible MSE**: Error on sampled positions (training data)\n2. **Invisible MSE**: Error on unseen positions (interpolation quality)\n3. **Interpolation Ratio**: invisible_mse / visible_mse (lower = better)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PIXNERD_DIR = NOTEBOOK_DIR / \"PixNerd\"\n",
    "\n",
    "if PIXNERD_DIR.exists():\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    sys.path.insert(0, str(PIXNERD_DIR))\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"ERROR: PixNerd directory not found at {PIXNERD_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Must match training!\n# =============================================================================\n\nCHECKPOINT_PATH = str(NOTEBOOK_DIR / \"workdirs/exp_sinusoidal_nf_test/checkpoints/last.ckpt\")\n\n# Dataset config\nRESOLUTION = 64  # High-res target\nCHANNELS = 1\nNUM_COMPONENTS = 5\n\n# Super-resolution config (must match training)\nDOWNSAMPLE_FACTOR = 4  # 4x super-res: every 4th pixel visible\nMASK_MODE = \"grid\"  # 2D grid sampling\n\n# Model config\nPATCH_SIZE = 4\nHIDDEN_SIZE = 256\nDECODER_HIDDEN_SIZE = 64\nNUM_ENCODER_BLOCKS = 6\nNUM_DECODER_BLOCKS = 2\nNUM_GROUPS = 4\n\n# Sampling\nNUM_SAMPLE_STEPS = 50\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Compute visible ratio\nLOW_RES = RESOLUTION // DOWNSAMPLE_FACTOR\nif MASK_MODE == \"grid\":\n    VISIBLE_RATIO = 1.0 / (DOWNSAMPLE_FACTOR ** 2)\nelse:\n    VISIBLE_RATIO = 1.0 / DOWNSAMPLE_FACTOR\n\nprint(f\"Checkpoint: {CHECKPOINT_PATH}\")\nprint(f\"High-res target: {RESOLUTION}x{RESOLUTION}\")\nprint(f\"Low-res input: {LOW_RES}x{LOW_RES}\")\nprint(f\"Super-resolution: {DOWNSAMPLE_FACTOR}x\")\nprint(f\"Visible ratio: {VISIBLE_RATIO:.1%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.transformer.pixnerd_c2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "\n",
    "# Import dataset utilities\n",
    "sys.path.insert(0, str(NOTEBOOK_DIR))\n",
    "from train_sinusoidal import SinusoidalPLModule, SinusoidalLightningModel, SimpleEMA\n",
    "from train_sinusoidal import UnconditionalConditioner, MaskedFlowMatchingTrainer\n",
    "from PixNerd.src.data.dataset.sinusoidal import (\n",
    "    generate_sinusoidal_image,\n",
    "    create_visibility_mask,\n",
    "    compute_interpolation_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    scheduler = LinearScheduler()\n",
    "    vae = PixelAE(scale=1.0)\n",
    "    conditioner = UnconditionalConditioner()\n",
    "    \n",
    "    denoiser = PixNerDiT(\n",
    "        in_channels=CHANNELS,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_groups=NUM_GROUPS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        decoder_hidden_size=DECODER_HIDDEN_SIZE,\n",
    "        num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
    "        num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
    "        num_classes=1,\n",
    "    )\n",
    "    \n",
    "    trainer = MaskedFlowMatchingTrainer(\n",
    "        scheduler=scheduler,\n",
    "        lognorm_t=True,\n",
    "        timeshift=1.0,\n",
    "    )\n",
    "    \n",
    "    sampler = EulerSampler(\n",
    "        num_steps=NUM_SAMPLE_STEPS,\n",
    "        guidance=1.0,\n",
    "        guidance_interval_min=0.0,\n",
    "        guidance_interval_max=1.0,\n",
    "        scheduler=scheduler,\n",
    "        w_scheduler=LinearScheduler(),\n",
    "        guidance_fn=simple_guidance_fn,\n",
    "        step_fn=ode_step_fn,\n",
    "    )\n",
    "    \n",
    "    ema_tracker = SimpleEMA(decay=0.9999)\n",
    "    \n",
    "    model = SinusoidalLightningModel(\n",
    "        vae=vae,\n",
    "        conditioner=conditioner,\n",
    "        denoiser=denoiser,\n",
    "        diffusion_trainer=trainer,\n",
    "        diffusion_sampler=sampler,\n",
    "        ema_tracker=ema_tracker,\n",
    "        optimizer=None,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = build_model()\n",
    "model.setup_ema()\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {CHECKPOINT_PATH}\")\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    # Extract model state from Lightning checkpoint\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "    # Remove 'model.' prefix if present\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('model.'):\n",
    "            new_state_dict[k[6:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"Checkpoint loaded!\")\n",
    "else:\n",
    "    print(f\"WARNING: Checkpoint not found!\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.denoiser.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Create Visibility Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Create visibility mask (regular grid sampling)\nvisibility_mask = create_visibility_mask(\n    RESOLUTION, DOWNSAMPLE_FACTOR, MASK_MODE\n)\n\nvisible_ratio = visibility_mask.sum() / visibility_mask.size\n\nprint(f\"Visibility mask shape: {visibility_mask.shape}\")\nprint(f\"Visible pixels: {visibility_mask.sum()} / {visibility_mask.size} ({visible_ratio:.1%})\")\nprint(f\"This simulates {LOW_RES}x{LOW_RES} → {RESOLUTION}x{RESOLUTION} super-resolution\")\n\n# Visualize mask\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Full mask\naxes[0].imshow(visibility_mask, cmap='RdYlGn', interpolation='nearest')\naxes[0].set_title(f\"Visibility Mask ({visible_ratio:.1%} visible = green)\")\n\n# Zoomed view to show grid pattern\nzoom_size = 16\naxes[1].imshow(visibility_mask[:zoom_size, :zoom_size], cmap='RdYlGn', interpolation='nearest')\naxes[1].set_title(f\"Zoomed View (top-left {zoom_size}x{zoom_size})\")\nfor ax in axes:\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nGrid pattern: Every {DOWNSAMPLE_FACTOR}th pixel in both x and y directions\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Generate Ground Truth Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_samples(num_samples=10, seed_start=1000):\n",
    "    \"\"\"Generate ground truth sinusoidal images for testing.\"\"\"\n",
    "    samples = []\n",
    "    for i in range(num_samples):\n",
    "        img = generate_sinusoidal_image(\n",
    "            RESOLUTION, NUM_COMPONENTS,\n",
    "            freq_range=(1.0, 8.0),\n",
    "            seed=seed_start + i\n",
    "        )\n",
    "        samples.append(img)\n",
    "    return np.stack(samples)\n",
    "\n",
    "\n",
    "# Generate test samples\n",
    "test_samples = generate_test_samples(10)\n",
    "print(f\"Generated {len(test_samples)} test samples\")\n",
    "\n",
    "# Display a few\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, test_samples)):\n",
    "    ax.imshow(img, cmap='viridis')\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Ground Truth Sinusoidal Patterns\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Reconstruction Test\n",
    "\n",
    "Test 1: Given a noisy version of a sinusoidal image, can the model reconstruct it?\n",
    "\n",
    "The model was trained only on visible regions, but must predict ALL pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reconstruct_from_noise(model, num_samples=4):\n",
    "    \"\"\"Generate samples from pure noise.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Random noise\n",
    "    noise = torch.randn(num_samples, CHANNELS, RESOLUTION, RESOLUTION, device=DEVICE)\n",
    "    \n",
    "    # Get condition (dummy)\n",
    "    condition, uncondition = model.conditioner(noise)\n",
    "    \n",
    "    # Sample using EMA model\n",
    "    denoiser = model.ema_denoiser if model.ema_denoiser is not None else model.denoiser\n",
    "    \n",
    "    samples = model.diffusion_sampler(\n",
    "        denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    images = model.vae.decode(samples)\n",
    "    \n",
    "    return images.clamp(0, 1).cpu().numpy()\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "print(\"Generating samples from noise...\")\n",
    "generated = reconstruct_from_noise(model, num_samples=8)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, generated)):\n",
    "    ax.imshow(img[0], cmap='viridis')  # [C, H, W] -> [H, W]\n",
    "    ax.set_title(f\"Generated {i}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Generated Samples from Noise\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Interpolation Quality Analysis\n",
    "\n",
    "Key question: How well does the model predict UNSEEN regions?\n",
    "\n",
    "We compare:\n",
    "1. MSE on visible regions (training data)\n",
    "2. MSE on invisible regions (interpolation quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_interpolation(ground_truth, generated, mask):\n",
    "    \"\"\"\n",
    "    Visualize interpolation quality.\n",
    "    \n",
    "    Shows:\n",
    "    - Ground truth\n",
    "    - Generated image\n",
    "    - Error map\n",
    "    - Visible/invisible region errors\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0, 0].imshow(ground_truth, cmap='viridis')\n",
    "    axes[0, 0].set_title(\"Ground Truth\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Generated\n",
    "    axes[0, 1].imshow(generated, cmap='viridis')\n",
    "    axes[0, 1].set_title(\"Generated\")\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Error map\n",
    "    error = np.abs(ground_truth - generated)\n",
    "    im = axes[0, 2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[0, 2].set_title(\"Absolute Error\")\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[0, 2])\n",
    "    \n",
    "    # Visibility mask overlay\n",
    "    overlay = np.stack([ground_truth, ground_truth, ground_truth], axis=-1)\n",
    "    overlay[~mask] = [1, 0, 0]  # Red for invisible regions\n",
    "    axes[1, 0].imshow(overlay * 0.5 + 0.5 * ground_truth[..., np.newaxis])\n",
    "    axes[1, 0].set_title(\"Visibility Mask (red = unseen)\")\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Error on visible regions only\n",
    "    visible_error = error.copy()\n",
    "    visible_error[~mask] = 0\n",
    "    im = axes[1, 1].imshow(visible_error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    visible_mse = (error[mask] ** 2).mean()\n",
    "    axes[1, 1].set_title(f\"Error on VISIBLE ({visible_mse:.4f} MSE)\")\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Error on invisible regions only\n",
    "    invisible_error = error.copy()\n",
    "    invisible_error[mask] = 0\n",
    "    im = axes[1, 2].imshow(invisible_error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    invisible_mse = (error[~mask] ** 2).mean()\n",
    "    axes[1, 2].set_title(f\"Error on INVISIBLE ({invisible_mse:.4f} MSE)\")\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    ratio = invisible_mse / (visible_mse + 1e-8)\n",
    "    plt.suptitle(f\"Interpolation Quality - Ratio: {ratio:.2f}x (lower = better)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return visible_mse, invisible_mse, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Analyze interpolation quality on generated samples\n# Visualize the generated samples with the grid mask overlay\n\nfor i in range(min(4, len(generated))):\n    gen_img = generated[i, 0]  # [H, W]\n    \n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    # Generated image\n    axes[0].imshow(gen_img, cmap='viridis')\n    axes[0].set_title(\"Generated\")\n    axes[0].axis('off')\n    \n    # With mask overlay (show sampled grid points)\n    overlay = gen_img.copy()\n    axes[1].imshow(overlay, cmap='viridis')\n    # Mark visible (sampled) positions\n    y_vis, x_vis = np.where(visibility_mask)\n    axes[1].scatter(x_vis, y_vis, c='red', s=2, alpha=0.5)\n    axes[1].set_title(f\"Sampled positions (red dots, {DOWNSAMPLE_FACTOR}x grid)\")\n    axes[1].axis('off')\n    \n    # Gradient magnitude (smoothness check)\n    grad_x = np.abs(np.diff(gen_img, axis=1))\n    grad_y = np.abs(np.diff(gen_img, axis=0))\n    axes[2].imshow(grad_x[:-1, :], cmap='hot')\n    axes[2].set_title(\"Horizontal Gradient (edges)\")\n    axes[2].axis('off')\n    \n    plt.suptitle(f\"Sample {i} Analysis\")\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Boundary Analysis\n",
    "\n",
    "Check if there are visible artifacts at the boundaries between visible and invisible regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_boundaries(generated, mask):\n",
    "    \"\"\"\n",
    "    Analyze gradient magnitude at visibility boundaries.\n",
    "    \n",
    "    If interpolation is poor, we expect high gradients at boundaries.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img in generated:\n",
    "        img = img[0]  # [H, W]\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad_x = np.abs(np.diff(img, axis=1))\n",
    "        grad_x = np.pad(grad_x, ((0, 0), (0, 1)), mode='edge')\n",
    "        \n",
    "        # Find boundary pixels (where mask changes)\n",
    "        mask_diff = np.abs(np.diff(mask.astype(float), axis=1))\n",
    "        mask_diff = np.pad(mask_diff, ((0, 0), (0, 1)), mode='constant')\n",
    "        boundary_mask = mask_diff > 0\n",
    "        \n",
    "        # Gradient at boundaries vs elsewhere\n",
    "        grad_at_boundary = grad_x[boundary_mask].mean() if boundary_mask.any() else 0\n",
    "        grad_elsewhere = grad_x[~boundary_mask].mean()\n",
    "        \n",
    "        results.append({\n",
    "            'grad_at_boundary': grad_at_boundary,\n",
    "            'grad_elsewhere': grad_elsewhere,\n",
    "            'ratio': grad_at_boundary / (grad_elsewhere + 1e-8)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Analyze boundaries\n",
    "boundary_results = analyze_boundaries(generated, visibility_mask)\n",
    "\n",
    "print(\"Boundary Analysis:\")\n",
    "print(\"(Ratio close to 1.0 = good interpolation, no boundary artifacts)\")\n",
    "print()\n",
    "for i, r in enumerate(boundary_results):\n",
    "    print(f\"Sample {i}: boundary_grad={r['grad_at_boundary']:.4f}, \"\n",
    "          f\"other_grad={r['grad_elsewhere']:.4f}, ratio={r['ratio']:.2f}\")\n",
    "\n",
    "avg_ratio = np.mean([r['ratio'] for r in boundary_results])\n",
    "print(f\"\\nAverage boundary ratio: {avg_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## Summary\n\nThis benchmark tests neural field interpolation for super-resolution:\n\n1. **Training** on regularly sampled grid (simulating low-res input)\n2. **Evaluating** reconstruction quality on unseen positions\n3. **Checking** for artifacts in interpolated regions\n\n### Super-Resolution Setup\n- `downsample_factor=4`: 4x super-res (16x16 → 64x64)\n- Regular grid sampling (every 4th pixel in x and y)\n- ~6.25% visible pixels (256/4096)\n\n### Key Metrics:\n- **Interpolation ratio** < 2.0: Good interpolation\n- **Boundary ratio** ≈ 1.0: No grid artifacts\n- **Visual smoothness**: Generated patterns should be smooth sinusoids\n\n### Next Steps:\n- Try different downsample factors (2, 4, 8)\n- Test column-only or row-only sampling\n- Compare different model architectures"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}